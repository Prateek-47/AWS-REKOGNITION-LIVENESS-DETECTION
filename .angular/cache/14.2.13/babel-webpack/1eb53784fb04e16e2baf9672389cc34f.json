{"ast":null,"code":"import _asyncToGenerator from \"D:/aws-rekognition-liveness-detection-main/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContenxt, parseNodeName } from '../operations/executors/utils';\nimport { executeOp } from '../operations/operation_executor';\nimport { ExecutionContext } from './execution_context';\nimport { getExecutionSubgraph, getNodesInTopologicalOrder, isControlFlow } from './model_analysis';\nexport class GraphExecutor {\n  /**\n   *\n   * @param graph Graph the model or function graph to be executed.\n   * @param parent When building function exector you need to set the parent\n   * executor. Since the weights and function executor maps are set at parant\n   * level, that function executor can access the function maps and weight maps\n   * through the parent.\n   */\n  constructor(graph, parent) {\n    this.graph = graph;\n    this.parent = parent;\n    this.compiledMap = new Map();\n    this._weightMap = {};\n    this.SEPERATOR = ',';\n    this._functions = {};\n    this._functionExecutorMap = {};\n    this._outputs = graph.outputs;\n    this._inputs = graph.inputs;\n    this._initNodes = graph.initNodes;\n    this._signature = graph.signature;\n    this._functions = graph.functions; // create sub-graph executors\n\n    if (graph.functions != null) {\n      Object.keys(graph.functions).forEach(name => {\n        this._functionExecutorMap[name] = new GraphExecutor(graph.functions[name], this);\n      });\n    }\n  }\n\n  get weightIds() {\n    return this.parent ? this.parent.weightIds : this._weightIds;\n  }\n\n  get functionExecutorMap() {\n    return this.parent ? this.parent.functionExecutorMap : this._functionExecutorMap;\n  }\n\n  get weightMap() {\n    return this.parent ? this.parent.weightMap : this._weightMap;\n  }\n\n  set weightMap(weightMap) {\n    const weightIds = Object.keys(weightMap).map(key => weightMap[key].map(tensor => tensor.id));\n    this._weightIds = [].concat(...weightIds);\n    this._weightMap = weightMap;\n  }\n  /**\n   * Set `ResourceManager` shared by executors of a model.\n   * @param resourceManager: `ResourceManager` of the `GraphModel`.\n   */\n\n\n  set resourceManager(resourceManager) {\n    this._resourceManager = resourceManager;\n  }\n\n  get inputs() {\n    return this._inputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n        dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n      };\n    });\n  }\n\n  get outputs() {\n    return this._outputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n        dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n      };\n    });\n  }\n\n  get inputNodes() {\n    return this._inputs.map(node => node.signatureKey || node.name);\n  }\n\n  get outputNodes() {\n    return this._outputs.map(node => {\n      const name = node.signatureKey || node.name;\n      return node.defaultOutput ? `${name}:${node.defaultOutput}` : name;\n    });\n  }\n\n  get functions() {\n    return Object.keys(this._functions).reduce((map, key) => {\n      map[key] = this._functions[key].signature;\n      return map;\n    }, {});\n  }\n\n  getCompilationKey(inputs, outputs) {\n    const sortedInputs = inputs.map(node => node.name).sort();\n    const sortedOutputs = outputs.map(node => node.name).sort();\n    return sortedInputs.join(this.SEPERATOR) + '--' + sortedOutputs.join(this.SEPERATOR);\n  }\n  /**\n   * Compiles the inference graph and returns the minimal set of nodes that are\n   * required for execution, in the correct execution order.\n   */\n\n\n  compile(inputs, outputs) {\n    const executionInfo = getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n    const {\n      missingInputs,\n      dynamicNode,\n      syncInputs\n    } = executionInfo;\n\n    if (dynamicNode != null) {\n      throw new Error(`This execution contains the node '${dynamicNode.name}', which has ` + `the dynamic op '${dynamicNode.op}'. Please use ` + `model.executeAsync() instead. Alternatively, to avoid the ` + `dynamic ops, specify the inputs [${syncInputs}]`);\n    }\n\n    if (missingInputs.length > 0) {\n      const outNames = outputs.map(n => n.name);\n      const inNames = Object.keys(inputs);\n      throw new Error(`Cannot compute the outputs [${outNames}] from the provided inputs ` + `[${inNames}]. Missing the following inputs: [${missingInputs}]`);\n    }\n\n    return getNodesInTopologicalOrder(this.graph, this.weightMap, executionInfo);\n  }\n  /**\n   * Executes the inference for given input tensors.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model, if\n   * no outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   */\n\n\n  execute(inputs, outputs) {\n    inputs = this.mapInputs(inputs);\n    const names = Object.keys(inputs).sort();\n    this.checkInputs(inputs);\n    this.checkInputShapeAndType(inputs);\n    outputs = this.mapOutputs(outputs);\n    this.checkOutputs(outputs);\n    const inputNodes = names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputs.map(name => parseNodeName(name)[0]);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]); // If no outputs are specified, then use the default outputs of the model.\n\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const compilationKey = this.getCompilationKey(inputNodes, outputNodes); // Do nothing if the compiled graph cache contains the input.\n\n    let orderedNodes = this.compiledMap.get(compilationKey);\n\n    if (orderedNodes == null) {\n      orderedNodes = this.compile(inputs, outputNodes);\n      this.compiledMap.set(compilationKey, orderedNodes);\n    }\n\n    const tensorArrayMap = {};\n    const tensorListMap = {};\n    return tidy(() => {\n      const context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap);\n      const tensorsMap = Object.assign({}, this.weightMap);\n      Object.keys(inputs).forEach(name => {\n        const [nodeName, index] = parseNodeName(name);\n        const tensors = [];\n        tensors[index] = inputs[name];\n        tensorsMap[nodeName] = tensors;\n      });\n      const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n      const intermediateTensorConsumerCount = {};\n\n      for (let i = 0; i < orderedNodes.length; i++) {\n        const node = orderedNodes[i];\n\n        if (!tensorsMap[node.name]) {\n          const tensors = executeOp(node, tensorsMap, context, this._resourceManager);\n\n          if (util.isPromise(tensors)) {\n            throw new Error(`The execution of the op '${node.op}' returned a promise. ` + `Please use model.executeAsync() instead.`);\n          }\n\n          tensorsMap[node.name] = tensors;\n          this.checkTensorForDisposal(node.name, node, tensorsMap, context, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount);\n        }\n      } // dispose the context for the root executor\n\n\n      if (this.parent == null) {\n        context.dispose(tensorsToKeep);\n      }\n\n      return outputs.map(name => getTensor(name, tensorsMap, context));\n    });\n  }\n\n  getFrozenTensorIds(tensorMap) {\n    const ids = [].concat.apply([], Object.keys(tensorMap).map(key => tensorMap[key]).map(tensors => tensors.map(tensor => tensor.id)));\n    return new Set(ids);\n  }\n\n  checkTensorForDisposal(nodeName, node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount) {\n    // Skip output nodes and any control flow nodes, since its dependency is\n    // tricky to track correctly.\n    if (node.category === 'control' || outputNames.indexOf(nodeName) !== -1) {\n      return;\n    }\n\n    tensorMap[nodeName].forEach(tensor => {\n      if (tensor != null) {\n        intermediateTensorConsumerCount[tensor.id] = (intermediateTensorConsumerCount[tensor.id] || 0) + node.children.length;\n      }\n    });\n    node.inputs.forEach(input => {\n      // Skip any control flow nodes, since its dependency is tricky to track\n      // correctly.\n      if (input.category !== 'control') {\n        const tensors = getTensorsForCurrentContenxt(input.name, tensorMap, context);\n\n        if (tensors != null) {\n          tensors.forEach(tensor => {\n            if (tensor && !tensor.kept && !tensorsToKeep.has(tensor.id)) {\n              const count = intermediateTensorConsumerCount[tensor.id];\n\n              if (count === 1) {\n                tensor.dispose();\n                delete intermediateTensorConsumerCount[tensor.id];\n              } else if (count != null) {\n                // only intermediate nodes has count set, inputs and weights are\n                // not.\n                intermediateTensorConsumerCount[tensor.id]--;\n              }\n            }\n          });\n        }\n      }\n    });\n  }\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs output node name from the Tensorflow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   */\n\n\n  executeAsync(inputs, outputs) {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      return _this._executeAsync(inputs, outputs);\n    })();\n  }\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   * @param isFunctionExecution Optional. Flag for executing a function.\n   * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n   * function execution.\n   * @param tensorArrayMap Optinal global TensorList map by id. Used for\n   * function execution.\n   */\n\n\n  _executeAsync(inputs, outputs, isFunctionExecution = false, tensorArrayMap = {}, tensorListMap = {}) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      if (!isFunctionExecution) {\n        inputs = _this2.mapInputs(inputs);\n\n        _this2.checkInputs(inputs);\n\n        _this2.checkInputShapeAndType(inputs);\n\n        outputs = _this2.mapOutputs(outputs);\n\n        _this2.checkOutputs(outputs);\n      }\n\n      const context = new ExecutionContext(_this2.weightMap, tensorArrayMap, tensorListMap, _this2.functionExecutorMap); // Graph with control flow op requires runtime evaluation of the execution\n      // order, while without control flow the execution order is pre-determined\n      // in the compile method.\n\n      const tensorMap = yield _this2.executeWithControlFlow(inputs, context, outputs, isFunctionExecution);\n      const results = outputs.map(name => getTensor(name, tensorMap, context)); // dispose all the intermediate tensors\n\n      const outputIds = results.map(t => t.id);\n      const inputIds = Object.keys(inputs).map(name => inputs[name].id);\n      const keepIds = new Set([...outputIds, ...inputIds, ..._this2.weightIds]);\n      Object.keys(tensorMap).forEach(key => {\n        const tensorArray = tensorMap[key];\n        tensorArray.forEach(tensor => {\n          if (tensor && !tensor.kept && !tensor.isDisposed && !keepIds.has(tensor.id)) {\n            tensor.dispose();\n          }\n        });\n      }); // dispose the context for the root executor\n\n      if (_this2.parent == null) {\n        context.dispose(keepIds);\n      }\n\n      return results;\n    })();\n  }\n\n  executeFunctionAsync(inputs, tensorArrayMap, tensorListMap) {\n    var _this3 = this;\n\n    return _asyncToGenerator(function* () {\n      const mappedInputs = inputs.reduce((map, tensor, index) => {\n        map[_this3.inputs[index].name] = tensor;\n        return map;\n      }, {});\n      return _this3._executeAsync(mappedInputs, _this3.outputNodes, true, tensorArrayMap, tensorListMap);\n    })();\n  }\n  /**\n   * When there are control flow nodes in the graph, the graph execution use\n   * ExecutionContext to keep track of the frames and loop iterators.\n   * @param inputs placeholder tensors for the graph.\n   * @param context the execution context object for current execution.\n   * @param outputNames Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   * @param isFunctionExecution Flag for executing a function.\n   */\n\n\n  executeWithControlFlow(inputs, context, outputNames, isFunctionExecution) {\n    var _this4 = this;\n\n    return _asyncToGenerator(function* () {\n      const names = Object.keys(inputs);\n      const inputNodes = names.map(name => _this4.graph.nodes[parseNodeName(name)[0]]);\n      const outputNodeNames = outputNames.map(name => parseNodeName(name)[0]);\n      let outputNodes = outputNodeNames.map(name => _this4.graph.nodes[name]); // If no outputs are specified, then use the default outputs of the model.\n\n      if (outputNodes.length === 0) {\n        outputNodes = _this4._outputs;\n      }\n\n      const {\n        usedNodes,\n        missingInputs,\n        dynamicNode,\n        syncInputs\n      } = getExecutionSubgraph(inputs, outputNodes, _this4.weightMap, _this4._initNodes); // First nodes to execute include inputNodes, weights, and initNodes.\n\n      const stack = [...inputNodes, ..._this4.graph.weights, ...(_this4._initNodes || [])].map(node => {\n        return {\n          node,\n          contexts: context.currentContext\n        };\n      });\n      const tensorsMap = Object.assign({}, _this4.weightMap);\n      Object.keys(inputs).forEach(name => {\n        const [nodeName, index] = parseNodeName(name);\n        const tensors = [];\n        tensors[index] = inputs[name];\n        tensorsMap[nodeName] = tensors;\n      });\n      const intermediateTensorConsumerCount = {};\n\n      const tensorsToKeep = _this4.getFrozenTensorIds(tensorsMap);\n\n      const added = {};\n\n      while (stack.length > 0) {\n        const promises = _this4.processStack(inputNodes, stack, context, tensorsMap, added, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount, usedNodes);\n\n        yield Promise.all(promises);\n      }\n\n      if (dynamicNode == null && !isFunctionExecution) {\n        console.warn(`This model execution did not contain any nodes with control flow ` + `or dynamic output shapes. You can use model.execute() instead.`);\n      }\n\n      const missingOutputs = outputNodes.filter(node => !isControlFlow(node) && !getTensor(node.name, tensorsMap, context)).map(node => node.name);\n\n      if (missingOutputs.length > 0) {\n        let alternativeMsg = '';\n\n        if (dynamicNode != null) {\n          alternativeMsg = `Alternatively, to avoid the dynamic ops, use model.execute() ` + `and specify the inputs [${syncInputs}]`;\n        }\n\n        throw new Error(`Cannot compute the outputs [${missingOutputs}] from the provided ` + `inputs [${names}]. Consider providing the following inputs: ` + `[${missingInputs}]. ${alternativeMsg}`);\n      }\n\n      return tensorsMap;\n    })();\n  }\n\n  processStack(inputNodes, stack, context, tensorMap, added, tensorsToKeep, outputNames, intermediateTensorConsumerCount, usedNodes) {\n    const promises = [];\n\n    while (stack.length > 0) {\n      const item = stack.pop();\n      context.currentContext = item.contexts;\n      let nodeName = ''; // The tensor of the Enter op with isConstant set should be set\n      // in the parent scope, so it will be available as constant for the\n      // whole loop.\n\n      if (item.node.op === 'Enter' && getParamValue('isConstant', item.node, tensorMap, context)) {\n        [nodeName] = getNodeNameAndIndex(item.node.name, context);\n      } // only process nodes that are not in the tensorMap yet, this include\n      // inputNodes and internal initNodes.\n\n\n      if (tensorMap[item.node.name] == null) {\n        const tensors = executeOp(item.node, tensorMap, context, this._resourceManager);\n\n        if (!nodeName) {\n          [nodeName] = getNodeNameAndIndex(item.node.name, context);\n        }\n\n        const currentContext = context.currentContext;\n\n        if (util.isPromise(tensors)) {\n          promises.push(tensors.then(t => {\n            tensorMap[nodeName] = t;\n            context.currentContext = currentContext;\n            this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n            this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n            return t;\n          }));\n        } else {\n          tensorMap[nodeName] = tensors;\n          this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n          this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n        }\n      } else {\n        this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n      }\n    }\n\n    return promises;\n  }\n\n  processChildNodes(node, stack, context, tensorMap, added, usedNodes) {\n    node.children.forEach(childNode => {\n      const [nodeName] = getNodeNameAndIndex(childNode.name, context);\n\n      if (added[nodeName] || !usedNodes.has(childNode.name)) {\n        return;\n      } // Merge op can be pushed if any of its inputs has value.\n\n\n      if (childNode.op === 'Merge') {\n        if (childNode.inputNames.some(name => {\n          return !!getTensor(name, tensorMap, context);\n        })) {\n          added[nodeName] = true;\n          stack.push({\n            contexts: context.currentContext,\n            node: childNode\n          });\n        }\n      } else // Otherwise all inputs must to have value.\n        if (childNode.inputNames.every(name => {\n          return !!getTensor(name, tensorMap, context);\n        })) {\n          added[nodeName] = true;\n          stack.push({\n            contexts: context.currentContext,\n            node: childNode\n          });\n        }\n    });\n  }\n  /**\n   * Releases the memory used by the weight tensors.\n   */\n\n\n  dispose() {\n    Object.keys(this.weightMap).forEach(key => this.weightMap[key].forEach(tensor => tensor.dispose()));\n  }\n\n  checkInputShapeAndType(inputs) {\n    Object.keys(inputs).forEach(name => {\n      const input = inputs[name];\n      const [nodeName] = parseNodeName(name);\n      const node = this.graph.nodes[nodeName];\n\n      if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n        const shape = node.attrParams['shape'].value;\n        const match = shape.length === input.shape.length && input.shape.every((dim, index) => shape[index] === -1 || shape[index] === dim);\n        util.assert(match, () => `The shape of dict['${node.name}'] provided in ` + `model.execute(dict) must be [${shape}], but was ` + `[${input.shape}]`);\n      }\n\n      if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n        util.assert(input.dtype === node.attrParams['dtype'].value, () => `The dtype of dict['${node.name}'] provided in ` + `model.execute(dict) must be ` + `${node.attrParams['dtype'].value}, but was ${input.dtype}`);\n      }\n    });\n  }\n\n  mapInputs(inputs) {\n    const result = {};\n\n    for (const inputName in inputs) {\n      if (this._signature != null && this._signature.inputs != null && this._signature.inputs[inputName] != null) {\n        const tensor = this._signature.inputs[inputName];\n        result[tensor.name] = inputs[inputName];\n      } else {\n        result[inputName] = inputs[inputName];\n      }\n    }\n\n    return result;\n  }\n\n  checkInputs(inputs) {\n    const notInGraph = Object.keys(inputs).filter(name => {\n      const [nodeName] = parseNodeName(name);\n      return this.graph.nodes[nodeName] == null;\n    });\n\n    if (notInGraph.length > 0) {\n      throw new Error(`The dict provided in model.execute(dict) has ` + `keys: [${notInGraph}] that are not part of graph`);\n    }\n  }\n\n  mapOutputs(outputs) {\n    return outputs.map(name => {\n      if (this._signature != null && this._signature.outputs != null && this._signature.outputs[name] != null) {\n        const tensor = this._signature.outputs[name];\n        return tensor.name;\n      }\n\n      return name;\n    }, {});\n  }\n\n  checkOutputs(outputs) {\n    outputs.forEach(name => {\n      const [normalizedName] = parseNodeName(name);\n\n      if (!this.graph.nodes[normalizedName]) {\n        throw new Error(`The output '${name}' is not found in the graph`);\n      }\n    });\n  }\n\n}","map":{"version":3,"names":["tidy","util","getNodeNameAndIndex","getParamValue","getTensor","getTensorsForCurrentContenxt","parseNodeName","executeOp","ExecutionContext","getExecutionSubgraph","getNodesInTopologicalOrder","isControlFlow","GraphExecutor","constructor","graph","parent","compiledMap","Map","_weightMap","SEPERATOR","_functions","_functionExecutorMap","_outputs","outputs","_inputs","inputs","_initNodes","initNodes","_signature","signature","functions","Object","keys","forEach","name","weightIds","_weightIds","functionExecutorMap","weightMap","map","key","tensor","id","concat","resourceManager","_resourceManager","node","shape","attrParams","value","undefined","dtype","inputNodes","signatureKey","outputNodes","defaultOutput","reduce","getCompilationKey","sortedInputs","sort","sortedOutputs","join","compile","executionInfo","missingInputs","dynamicNode","syncInputs","Error","op","length","outNames","n","inNames","execute","mapInputs","names","checkInputs","checkInputShapeAndType","mapOutputs","checkOutputs","nodes","outputNodeNames","compilationKey","orderedNodes","get","set","tensorArrayMap","tensorListMap","context","tensorsMap","assign","nodeName","index","tensors","tensorsToKeep","getFrozenTensorIds","intermediateTensorConsumerCount","i","isPromise","checkTensorForDisposal","dispose","tensorMap","ids","apply","Set","outputNames","category","indexOf","children","input","kept","has","count","executeAsync","_executeAsync","isFunctionExecution","executeWithControlFlow","results","outputIds","t","inputIds","keepIds","tensorArray","isDisposed","executeFunctionAsync","mappedInputs","usedNodes","stack","weights","contexts","currentContext","added","promises","processStack","Promise","all","console","warn","missingOutputs","filter","alternativeMsg","item","pop","push","then","processChildNodes","childNode","inputNames","some","every","match","dim","assert","result","inputName","notInGraph","normalizedName"],"sources":["D:/aws-rekognition-liveness-detection-main/node_modules/@tensorflow/tfjs-converter/dist/executor/graph_executor.js"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContenxt, parseNodeName } from '../operations/executors/utils';\nimport { executeOp } from '../operations/operation_executor';\nimport { ExecutionContext } from './execution_context';\nimport { getExecutionSubgraph, getNodesInTopologicalOrder, isControlFlow } from './model_analysis';\nexport class GraphExecutor {\n    /**\n     *\n     * @param graph Graph the model or function graph to be executed.\n     * @param parent When building function exector you need to set the parent\n     * executor. Since the weights and function executor maps are set at parant\n     * level, that function executor can access the function maps and weight maps\n     * through the parent.\n     */\n    constructor(graph, parent) {\n        this.graph = graph;\n        this.parent = parent;\n        this.compiledMap = new Map();\n        this._weightMap = {};\n        this.SEPERATOR = ',';\n        this._functions = {};\n        this._functionExecutorMap = {};\n        this._outputs = graph.outputs;\n        this._inputs = graph.inputs;\n        this._initNodes = graph.initNodes;\n        this._signature = graph.signature;\n        this._functions = graph.functions;\n        // create sub-graph executors\n        if (graph.functions != null) {\n            Object.keys(graph.functions).forEach(name => {\n                this._functionExecutorMap[name] =\n                    new GraphExecutor(graph.functions[name], this);\n            });\n        }\n    }\n    get weightIds() {\n        return this.parent ? this.parent.weightIds : this._weightIds;\n    }\n    get functionExecutorMap() {\n        return this.parent ? this.parent.functionExecutorMap :\n            this._functionExecutorMap;\n    }\n    get weightMap() {\n        return this.parent ? this.parent.weightMap : this._weightMap;\n    }\n    set weightMap(weightMap) {\n        const weightIds = Object.keys(weightMap).map(key => weightMap[key].map(tensor => tensor.id));\n        this._weightIds = [].concat(...weightIds);\n        this._weightMap = weightMap;\n    }\n    /**\n     * Set `ResourceManager` shared by executors of a model.\n     * @param resourceManager: `ResourceManager` of the `GraphModel`.\n     */\n    set resourceManager(resourceManager) {\n        this._resourceManager = resourceManager;\n    }\n    get inputs() {\n        return this._inputs.map(node => {\n            return {\n                name: node.name,\n                shape: node.attrParams['shape'] ?\n                    node.attrParams['shape'].value :\n                    undefined,\n                dtype: node.attrParams['dtype'] ?\n                    node.attrParams['dtype'].value :\n                    undefined\n            };\n        });\n    }\n    get outputs() {\n        return this._outputs.map(node => {\n            return {\n                name: node.name,\n                shape: node.attrParams['shape'] ?\n                    node.attrParams['shape'].value :\n                    undefined,\n                dtype: node.attrParams['dtype'] ?\n                    node.attrParams['dtype'].value :\n                    undefined\n            };\n        });\n    }\n    get inputNodes() {\n        return this._inputs.map(node => node.signatureKey || node.name);\n    }\n    get outputNodes() {\n        return this._outputs.map((node) => {\n            const name = node.signatureKey || node.name;\n            return node.defaultOutput ? (`${name}:${node.defaultOutput}`) : name;\n        });\n    }\n    get functions() {\n        return Object.keys(this._functions).reduce((map, key) => {\n            map[key] = this._functions[key].signature;\n            return map;\n        }, {});\n    }\n    getCompilationKey(inputs, outputs) {\n        const sortedInputs = inputs.map(node => node.name).sort();\n        const sortedOutputs = outputs.map(node => node.name).sort();\n        return sortedInputs.join(this.SEPERATOR) + '--' +\n            sortedOutputs.join(this.SEPERATOR);\n    }\n    /**\n     * Compiles the inference graph and returns the minimal set of nodes that are\n     * required for execution, in the correct execution order.\n     */\n    compile(inputs, outputs) {\n        const executionInfo = getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n        const { missingInputs, dynamicNode, syncInputs } = executionInfo;\n        if (dynamicNode != null) {\n            throw new Error(`This execution contains the node '${dynamicNode.name}', which has ` +\n                `the dynamic op '${dynamicNode.op}'. Please use ` +\n                `model.executeAsync() instead. Alternatively, to avoid the ` +\n                `dynamic ops, specify the inputs [${syncInputs}]`);\n        }\n        if (missingInputs.length > 0) {\n            const outNames = outputs.map(n => n.name);\n            const inNames = Object.keys(inputs);\n            throw new Error(`Cannot compute the outputs [${outNames}] from the provided inputs ` +\n                `[${inNames}]. Missing the following inputs: [${missingInputs}]`);\n        }\n        return getNodesInTopologicalOrder(this.graph, this.weightMap, executionInfo);\n    }\n    /**\n     * Executes the inference for given input tensors.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs Optional. output node name from the Tensorflow model, if\n     * no outputs are specified, the default outputs of the model would be used.\n     * You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     */\n    execute(inputs, outputs) {\n        inputs = this.mapInputs(inputs);\n        const names = Object.keys(inputs).sort();\n        this.checkInputs(inputs);\n        this.checkInputShapeAndType(inputs);\n        outputs = this.mapOutputs(outputs);\n        this.checkOutputs(outputs);\n        const inputNodes = names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n        const outputNodeNames = outputs.map(name => parseNodeName(name)[0]);\n        let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n        // If no outputs are specified, then use the default outputs of the model.\n        if (outputNodes.length === 0) {\n            outputNodes = this._outputs;\n        }\n        const compilationKey = this.getCompilationKey(inputNodes, outputNodes);\n        // Do nothing if the compiled graph cache contains the input.\n        let orderedNodes = this.compiledMap.get(compilationKey);\n        if (orderedNodes == null) {\n            orderedNodes = this.compile(inputs, outputNodes);\n            this.compiledMap.set(compilationKey, orderedNodes);\n        }\n        const tensorArrayMap = {};\n        const tensorListMap = {};\n        return tidy(() => {\n            const context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap);\n            const tensorsMap = Object.assign({}, this.weightMap);\n            Object.keys(inputs).forEach(name => {\n                const [nodeName, index] = parseNodeName(name);\n                const tensors = [];\n                tensors[index] = inputs[name];\n                tensorsMap[nodeName] = tensors;\n            });\n            const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n            const intermediateTensorConsumerCount = {};\n            for (let i = 0; i < orderedNodes.length; i++) {\n                const node = orderedNodes[i];\n                if (!tensorsMap[node.name]) {\n                    const tensors = executeOp(node, tensorsMap, context, this._resourceManager);\n                    if (util.isPromise(tensors)) {\n                        throw new Error(`The execution of the op '${node.op}' returned a promise. ` +\n                            `Please use model.executeAsync() instead.`);\n                    }\n                    tensorsMap[node.name] = tensors;\n                    this.checkTensorForDisposal(node.name, node, tensorsMap, context, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount);\n                }\n            }\n            // dispose the context for the root executor\n            if (this.parent == null) {\n                context.dispose(tensorsToKeep);\n            }\n            return outputs.map(name => getTensor(name, tensorsMap, context));\n        });\n    }\n    getFrozenTensorIds(tensorMap) {\n        const ids = [].concat.apply([], Object.keys(tensorMap)\n            .map(key => tensorMap[key])\n            .map(tensors => tensors.map(tensor => tensor.id)));\n        return new Set(ids);\n    }\n    checkTensorForDisposal(nodeName, node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount) {\n        // Skip output nodes and any control flow nodes, since its dependency is\n        // tricky to track correctly.\n        if (node.category === 'control' || outputNames.indexOf(nodeName) !== -1) {\n            return;\n        }\n        tensorMap[nodeName].forEach(tensor => {\n            if (tensor != null) {\n                intermediateTensorConsumerCount[tensor.id] =\n                    (intermediateTensorConsumerCount[tensor.id] || 0) +\n                        node.children.length;\n            }\n        });\n        node.inputs.forEach(input => {\n            // Skip any control flow nodes, since its dependency is tricky to track\n            // correctly.\n            if (input.category !== 'control') {\n                const tensors = getTensorsForCurrentContenxt(input.name, tensorMap, context);\n                if (tensors != null) {\n                    tensors.forEach(tensor => {\n                        if (tensor && !tensor.kept && !tensorsToKeep.has(tensor.id)) {\n                            const count = intermediateTensorConsumerCount[tensor.id];\n                            if (count === 1) {\n                                tensor.dispose();\n                                delete intermediateTensorConsumerCount[tensor.id];\n                            }\n                            else if (count != null) {\n                                // only intermediate nodes has count set, inputs and weights are\n                                // not.\n                                intermediateTensorConsumerCount[tensor.id]--;\n                            }\n                        }\n                    });\n                }\n            }\n        });\n    }\n    /**\n     * Executes the inference for given input tensors in Async fashion.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs output node name from the Tensorflow model, if no outputs\n     * are specified, the default outputs of the model would be used. You can\n     * inspect intermediate nodes of the model by adding them to the outputs\n     * array.\n     */\n    async executeAsync(inputs, outputs) {\n        return this._executeAsync(inputs, outputs);\n    }\n    /**\n     * Executes the inference for given input tensors in Async fashion.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs Optional. output node name from the Tensorflow model,\n     * if no outputs are specified, the default outputs of the model would be\n     * used. You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     * @param isFunctionExecution Optional. Flag for executing a function.\n     * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n     * function execution.\n     * @param tensorArrayMap Optinal global TensorList map by id. Used for\n     * function execution.\n     */\n    async _executeAsync(inputs, outputs, isFunctionExecution = false, tensorArrayMap = {}, tensorListMap = {}) {\n        if (!isFunctionExecution) {\n            inputs = this.mapInputs(inputs);\n            this.checkInputs(inputs);\n            this.checkInputShapeAndType(inputs);\n            outputs = this.mapOutputs(outputs);\n            this.checkOutputs(outputs);\n        }\n        const context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap);\n        // Graph with control flow op requires runtime evaluation of the execution\n        // order, while without control flow the execution order is pre-determined\n        // in the compile method.\n        const tensorMap = await this.executeWithControlFlow(inputs, context, outputs, isFunctionExecution);\n        const results = outputs.map(name => getTensor(name, tensorMap, context));\n        // dispose all the intermediate tensors\n        const outputIds = results.map(t => t.id);\n        const inputIds = Object.keys(inputs).map(name => inputs[name].id);\n        const keepIds = new Set([...outputIds, ...inputIds, ...this.weightIds]);\n        Object.keys(tensorMap).forEach(key => {\n            const tensorArray = tensorMap[key];\n            tensorArray.forEach(tensor => {\n                if (tensor && !tensor.kept && !tensor.isDisposed &&\n                    !keepIds.has(tensor.id)) {\n                    tensor.dispose();\n                }\n            });\n        });\n        // dispose the context for the root executor\n        if (this.parent == null) {\n            context.dispose(keepIds);\n        }\n        return results;\n    }\n    async executeFunctionAsync(inputs, tensorArrayMap, tensorListMap) {\n        const mappedInputs = inputs.reduce((map, tensor, index) => {\n            map[this.inputs[index].name] = tensor;\n            return map;\n        }, {});\n        return this._executeAsync(mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap);\n    }\n    /**\n     * When there are control flow nodes in the graph, the graph execution use\n     * ExecutionContext to keep track of the frames and loop iterators.\n     * @param inputs placeholder tensors for the graph.\n     * @param context the execution context object for current execution.\n     * @param outputNames Optional. output node name from the Tensorflow model,\n     * if no outputs are specified, the default outputs of the model would be\n     * used. You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     * @param isFunctionExecution Flag for executing a function.\n     */\n    async executeWithControlFlow(inputs, context, outputNames, isFunctionExecution) {\n        const names = Object.keys(inputs);\n        const inputNodes = names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n        const outputNodeNames = outputNames.map(name => parseNodeName(name)[0]);\n        let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n        // If no outputs are specified, then use the default outputs of the model.\n        if (outputNodes.length === 0) {\n            outputNodes = this._outputs;\n        }\n        const { usedNodes, missingInputs, dynamicNode, syncInputs } = getExecutionSubgraph(inputs, outputNodes, this.weightMap, this._initNodes);\n        // First nodes to execute include inputNodes, weights, and initNodes.\n        const stack = [\n            ...inputNodes, ...this.graph.weights, ...(this._initNodes || [])\n        ].map(node => {\n            return { node, contexts: context.currentContext };\n        });\n        const tensorsMap = Object.assign({}, this.weightMap);\n        Object.keys(inputs).forEach(name => {\n            const [nodeName, index] = parseNodeName(name);\n            const tensors = [];\n            tensors[index] = inputs[name];\n            tensorsMap[nodeName] = tensors;\n        });\n        const intermediateTensorConsumerCount = {};\n        const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n        const added = {};\n        while (stack.length > 0) {\n            const promises = this.processStack(inputNodes, stack, context, tensorsMap, added, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount, usedNodes);\n            await Promise.all(promises);\n        }\n        if (dynamicNode == null && !isFunctionExecution) {\n            console.warn(`This model execution did not contain any nodes with control flow ` +\n                `or dynamic output shapes. You can use model.execute() instead.`);\n        }\n        const missingOutputs = outputNodes\n            .filter(node => !isControlFlow(node) &&\n            !getTensor(node.name, tensorsMap, context))\n            .map(node => node.name);\n        if (missingOutputs.length > 0) {\n            let alternativeMsg = '';\n            if (dynamicNode != null) {\n                alternativeMsg =\n                    `Alternatively, to avoid the dynamic ops, use model.execute() ` +\n                        `and specify the inputs [${syncInputs}]`;\n            }\n            throw new Error(`Cannot compute the outputs [${missingOutputs}] from the provided ` +\n                `inputs [${names}]. Consider providing the following inputs: ` +\n                `[${missingInputs}]. ${alternativeMsg}`);\n        }\n        return tensorsMap;\n    }\n    processStack(inputNodes, stack, context, tensorMap, added, tensorsToKeep, outputNames, intermediateTensorConsumerCount, usedNodes) {\n        const promises = [];\n        while (stack.length > 0) {\n            const item = stack.pop();\n            context.currentContext = item.contexts;\n            let nodeName = '';\n            // The tensor of the Enter op with isConstant set should be set\n            // in the parent scope, so it will be available as constant for the\n            // whole loop.\n            if (item.node.op === 'Enter' &&\n                getParamValue('isConstant', item.node, tensorMap, context)) {\n                [nodeName] = getNodeNameAndIndex(item.node.name, context);\n            }\n            // only process nodes that are not in the tensorMap yet, this include\n            // inputNodes and internal initNodes.\n            if (tensorMap[item.node.name] == null) {\n                const tensors = executeOp(item.node, tensorMap, context, this._resourceManager);\n                if (!nodeName) {\n                    [nodeName] = getNodeNameAndIndex(item.node.name, context);\n                }\n                const currentContext = context.currentContext;\n                if (util.isPromise(tensors)) {\n                    promises.push(tensors.then(t => {\n                        tensorMap[nodeName] = t;\n                        context.currentContext = currentContext;\n                        this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n                        this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n                        return t;\n                    }));\n                }\n                else {\n                    tensorMap[nodeName] = tensors;\n                    this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n                    this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n                }\n            }\n            else {\n                this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n            }\n        }\n        return promises;\n    }\n    processChildNodes(node, stack, context, tensorMap, added, usedNodes) {\n        node.children.forEach((childNode) => {\n            const [nodeName,] = getNodeNameAndIndex(childNode.name, context);\n            if (added[nodeName] || !usedNodes.has(childNode.name)) {\n                return;\n            }\n            // Merge op can be pushed if any of its inputs has value.\n            if (childNode.op === 'Merge') {\n                if (childNode.inputNames.some(name => {\n                    return !!getTensor(name, tensorMap, context);\n                })) {\n                    added[nodeName] = true;\n                    stack.push({ contexts: context.currentContext, node: childNode });\n                }\n            }\n            else // Otherwise all inputs must to have value.\n             if (childNode.inputNames.every(name => {\n                return !!getTensor(name, tensorMap, context);\n            })) {\n                added[nodeName] = true;\n                stack.push({ contexts: context.currentContext, node: childNode });\n            }\n        });\n    }\n    /**\n     * Releases the memory used by the weight tensors.\n     */\n    dispose() {\n        Object.keys(this.weightMap)\n            .forEach(key => this.weightMap[key].forEach(tensor => tensor.dispose()));\n    }\n    checkInputShapeAndType(inputs) {\n        Object.keys(inputs).forEach(name => {\n            const input = inputs[name];\n            const [nodeName,] = parseNodeName(name);\n            const node = this.graph.nodes[nodeName];\n            if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n                const shape = node.attrParams['shape'].value;\n                const match = shape.length === input.shape.length &&\n                    input.shape.every((dim, index) => shape[index] === -1 || shape[index] === dim);\n                util.assert(match, () => `The shape of dict['${node.name}'] provided in ` +\n                    `model.execute(dict) must be [${shape}], but was ` +\n                    `[${input.shape}]`);\n            }\n            if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n                util.assert(input.dtype === node.attrParams['dtype'].value, () => `The dtype of dict['${node.name}'] provided in ` +\n                    `model.execute(dict) must be ` +\n                    `${node.attrParams['dtype'].value}, but was ${input.dtype}`);\n            }\n        });\n    }\n    mapInputs(inputs) {\n        const result = {};\n        for (const inputName in inputs) {\n            if (this._signature != null && this._signature.inputs != null &&\n                this._signature.inputs[inputName] != null) {\n                const tensor = this._signature.inputs[inputName];\n                result[tensor.name] = inputs[inputName];\n            }\n            else {\n                result[inputName] = inputs[inputName];\n            }\n        }\n        return result;\n    }\n    checkInputs(inputs) {\n        const notInGraph = Object.keys(inputs).filter(name => {\n            const [nodeName] = parseNodeName(name);\n            return this.graph.nodes[nodeName] == null;\n        });\n        if (notInGraph.length > 0) {\n            throw new Error(`The dict provided in model.execute(dict) has ` +\n                `keys: [${notInGraph}] that are not part of graph`);\n        }\n    }\n    mapOutputs(outputs) {\n        return outputs.map(name => {\n            if (this._signature != null && this._signature.outputs != null &&\n                this._signature.outputs[name] != null) {\n                const tensor = this._signature.outputs[name];\n                return tensor.name;\n            }\n            return name;\n        }, {});\n    }\n    checkOutputs(outputs) {\n        outputs.forEach(name => {\n            const [normalizedName] = parseNodeName(name);\n            if (!this.graph.nodes[normalizedName]) {\n                throw new Error(`The output '${name}' is not found in the graph`);\n            }\n        });\n    }\n}\n"],"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,IAAT,EAAeC,IAAf,QAA2B,uBAA3B;AACA,SAASC,mBAAT,EAA8BC,aAA9B,EAA6CC,SAA7C,EAAwDC,4BAAxD,EAAsFC,aAAtF,QAA2G,+BAA3G;AACA,SAASC,SAAT,QAA0B,kCAA1B;AACA,SAASC,gBAAT,QAAiC,qBAAjC;AACA,SAASC,oBAAT,EAA+BC,0BAA/B,EAA2DC,aAA3D,QAAgF,kBAAhF;AACA,OAAO,MAAMC,aAAN,CAAoB;EACvB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;EACIC,WAAW,CAACC,KAAD,EAAQC,MAAR,EAAgB;IACvB,KAAKD,KAAL,GAAaA,KAAb;IACA,KAAKC,MAAL,GAAcA,MAAd;IACA,KAAKC,WAAL,GAAmB,IAAIC,GAAJ,EAAnB;IACA,KAAKC,UAAL,GAAkB,EAAlB;IACA,KAAKC,SAAL,GAAiB,GAAjB;IACA,KAAKC,UAAL,GAAkB,EAAlB;IACA,KAAKC,oBAAL,GAA4B,EAA5B;IACA,KAAKC,QAAL,GAAgBR,KAAK,CAACS,OAAtB;IACA,KAAKC,OAAL,GAAeV,KAAK,CAACW,MAArB;IACA,KAAKC,UAAL,GAAkBZ,KAAK,CAACa,SAAxB;IACA,KAAKC,UAAL,GAAkBd,KAAK,CAACe,SAAxB;IACA,KAAKT,UAAL,GAAkBN,KAAK,CAACgB,SAAxB,CAZuB,CAavB;;IACA,IAAIhB,KAAK,CAACgB,SAAN,IAAmB,IAAvB,EAA6B;MACzBC,MAAM,CAACC,IAAP,CAAYlB,KAAK,CAACgB,SAAlB,EAA6BG,OAA7B,CAAqCC,IAAI,IAAI;QACzC,KAAKb,oBAAL,CAA0Ba,IAA1B,IACI,IAAItB,aAAJ,CAAkBE,KAAK,CAACgB,SAAN,CAAgBI,IAAhB,CAAlB,EAAyC,IAAzC,CADJ;MAEH,CAHD;IAIH;EACJ;;EACY,IAATC,SAAS,GAAG;IACZ,OAAO,KAAKpB,MAAL,GAAc,KAAKA,MAAL,CAAYoB,SAA1B,GAAsC,KAAKC,UAAlD;EACH;;EACsB,IAAnBC,mBAAmB,GAAG;IACtB,OAAO,KAAKtB,MAAL,GAAc,KAAKA,MAAL,CAAYsB,mBAA1B,GACH,KAAKhB,oBADT;EAEH;;EACY,IAATiB,SAAS,GAAG;IACZ,OAAO,KAAKvB,MAAL,GAAc,KAAKA,MAAL,CAAYuB,SAA1B,GAAsC,KAAKpB,UAAlD;EACH;;EACY,IAAToB,SAAS,CAACA,SAAD,EAAY;IACrB,MAAMH,SAAS,GAAGJ,MAAM,CAACC,IAAP,CAAYM,SAAZ,EAAuBC,GAAvB,CAA2BC,GAAG,IAAIF,SAAS,CAACE,GAAD,CAAT,CAAeD,GAAf,CAAmBE,MAAM,IAAIA,MAAM,CAACC,EAApC,CAAlC,CAAlB;IACA,KAAKN,UAAL,GAAkB,GAAGO,MAAH,CAAU,GAAGR,SAAb,CAAlB;IACA,KAAKjB,UAAL,GAAkBoB,SAAlB;EACH;EACD;AACJ;AACA;AACA;;;EACuB,IAAfM,eAAe,CAACA,eAAD,EAAkB;IACjC,KAAKC,gBAAL,GAAwBD,eAAxB;EACH;;EACS,IAANnB,MAAM,GAAG;IACT,OAAO,KAAKD,OAAL,CAAae,GAAb,CAAiBO,IAAI,IAAI;MAC5B,OAAO;QACHZ,IAAI,EAAEY,IAAI,CAACZ,IADR;QAEHa,KAAK,EAAED,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC,SAJD;QAKHC,KAAK,EAAEL,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC;MAPD,CAAP;IASH,CAVM,CAAP;EAWH;;EACU,IAAP3B,OAAO,GAAG;IACV,OAAO,KAAKD,QAAL,CAAciB,GAAd,CAAkBO,IAAI,IAAI;MAC7B,OAAO;QACHZ,IAAI,EAAEY,IAAI,CAACZ,IADR;QAEHa,KAAK,EAAED,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC,SAJD;QAKHC,KAAK,EAAEL,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC;MAPD,CAAP;IASH,CAVM,CAAP;EAWH;;EACa,IAAVE,UAAU,GAAG;IACb,OAAO,KAAK5B,OAAL,CAAae,GAAb,CAAiBO,IAAI,IAAIA,IAAI,CAACO,YAAL,IAAqBP,IAAI,CAACZ,IAAnD,CAAP;EACH;;EACc,IAAXoB,WAAW,GAAG;IACd,OAAO,KAAKhC,QAAL,CAAciB,GAAd,CAAmBO,IAAD,IAAU;MAC/B,MAAMZ,IAAI,GAAGY,IAAI,CAACO,YAAL,IAAqBP,IAAI,CAACZ,IAAvC;MACA,OAAOY,IAAI,CAACS,aAAL,GAAuB,GAAErB,IAAK,IAAGY,IAAI,CAACS,aAAc,EAApD,GAAyDrB,IAAhE;IACH,CAHM,CAAP;EAIH;;EACY,IAATJ,SAAS,GAAG;IACZ,OAAOC,MAAM,CAACC,IAAP,CAAY,KAAKZ,UAAjB,EAA6BoC,MAA7B,CAAoC,CAACjB,GAAD,EAAMC,GAAN,KAAc;MACrDD,GAAG,CAACC,GAAD,CAAH,GAAW,KAAKpB,UAAL,CAAgBoB,GAAhB,EAAqBX,SAAhC;MACA,OAAOU,GAAP;IACH,CAHM,EAGJ,EAHI,CAAP;EAIH;;EACDkB,iBAAiB,CAAChC,MAAD,EAASF,OAAT,EAAkB;IAC/B,MAAMmC,YAAY,GAAGjC,MAAM,CAACc,GAAP,CAAWO,IAAI,IAAIA,IAAI,CAACZ,IAAxB,EAA8ByB,IAA9B,EAArB;IACA,MAAMC,aAAa,GAAGrC,OAAO,CAACgB,GAAR,CAAYO,IAAI,IAAIA,IAAI,CAACZ,IAAzB,EAA+ByB,IAA/B,EAAtB;IACA,OAAOD,YAAY,CAACG,IAAb,CAAkB,KAAK1C,SAAvB,IAAoC,IAApC,GACHyC,aAAa,CAACC,IAAd,CAAmB,KAAK1C,SAAxB,CADJ;EAEH;EACD;AACJ;AACA;AACA;;;EACI2C,OAAO,CAACrC,MAAD,EAASF,OAAT,EAAkB;IACrB,MAAMwC,aAAa,GAAGtD,oBAAoB,CAACgB,MAAD,EAASF,OAAT,EAAkB,KAAKe,SAAvB,EAAkC,KAAKZ,UAAvC,CAA1C;IACA,MAAM;MAAEsC,aAAF;MAAiBC,WAAjB;MAA8BC;IAA9B,IAA6CH,aAAnD;;IACA,IAAIE,WAAW,IAAI,IAAnB,EAAyB;MACrB,MAAM,IAAIE,KAAJ,CAAW,qCAAoCF,WAAW,CAAC/B,IAAK,eAAtD,GACX,mBAAkB+B,WAAW,CAACG,EAAG,gBADtB,GAEX,4DAFW,GAGX,oCAAmCF,UAAW,GAH7C,CAAN;IAIH;;IACD,IAAIF,aAAa,CAACK,MAAd,GAAuB,CAA3B,EAA8B;MAC1B,MAAMC,QAAQ,GAAG/C,OAAO,CAACgB,GAAR,CAAYgC,CAAC,IAAIA,CAAC,CAACrC,IAAnB,CAAjB;MACA,MAAMsC,OAAO,GAAGzC,MAAM,CAACC,IAAP,CAAYP,MAAZ,CAAhB;MACA,MAAM,IAAI0C,KAAJ,CAAW,+BAA8BG,QAAS,6BAAxC,GACX,IAAGE,OAAQ,qCAAoCR,aAAc,GAD5D,CAAN;IAEH;;IACD,OAAOtD,0BAA0B,CAAC,KAAKI,KAAN,EAAa,KAAKwB,SAAlB,EAA6ByB,aAA7B,CAAjC;EACH;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACIU,OAAO,CAAChD,MAAD,EAASF,OAAT,EAAkB;IACrBE,MAAM,GAAG,KAAKiD,SAAL,CAAejD,MAAf,CAAT;IACA,MAAMkD,KAAK,GAAG5C,MAAM,CAACC,IAAP,CAAYP,MAAZ,EAAoBkC,IAApB,EAAd;IACA,KAAKiB,WAAL,CAAiBnD,MAAjB;IACA,KAAKoD,sBAAL,CAA4BpD,MAA5B;IACAF,OAAO,GAAG,KAAKuD,UAAL,CAAgBvD,OAAhB,CAAV;IACA,KAAKwD,YAAL,CAAkBxD,OAAlB;IACA,MAAM6B,UAAU,GAAGuB,KAAK,CAACpC,GAAN,CAAUL,IAAI,IAAI,KAAKpB,KAAL,CAAWkE,KAAX,CAAiB1E,aAAa,CAAC4B,IAAD,CAAb,CAAoB,CAApB,CAAjB,CAAlB,CAAnB;IACA,MAAM+C,eAAe,GAAG1D,OAAO,CAACgB,GAAR,CAAYL,IAAI,IAAI5B,aAAa,CAAC4B,IAAD,CAAb,CAAoB,CAApB,CAApB,CAAxB;IACA,IAAIoB,WAAW,GAAG2B,eAAe,CAAC1C,GAAhB,CAAoBL,IAAI,IAAI,KAAKpB,KAAL,CAAWkE,KAAX,CAAiB9C,IAAjB,CAA5B,CAAlB,CATqB,CAUrB;;IACA,IAAIoB,WAAW,CAACe,MAAZ,KAAuB,CAA3B,EAA8B;MAC1Bf,WAAW,GAAG,KAAKhC,QAAnB;IACH;;IACD,MAAM4D,cAAc,GAAG,KAAKzB,iBAAL,CAAuBL,UAAvB,EAAmCE,WAAnC,CAAvB,CAdqB,CAerB;;IACA,IAAI6B,YAAY,GAAG,KAAKnE,WAAL,CAAiBoE,GAAjB,CAAqBF,cAArB,CAAnB;;IACA,IAAIC,YAAY,IAAI,IAApB,EAA0B;MACtBA,YAAY,GAAG,KAAKrB,OAAL,CAAarC,MAAb,EAAqB6B,WAArB,CAAf;MACA,KAAKtC,WAAL,CAAiBqE,GAAjB,CAAqBH,cAArB,EAAqCC,YAArC;IACH;;IACD,MAAMG,cAAc,GAAG,EAAvB;IACA,MAAMC,aAAa,GAAG,EAAtB;IACA,OAAOvF,IAAI,CAAC,MAAM;MACd,MAAMwF,OAAO,GAAG,IAAIhF,gBAAJ,CAAqB,KAAK8B,SAA1B,EAAqCgD,cAArC,EAAqDC,aAArD,EAAoE,KAAKlD,mBAAzE,CAAhB;MACA,MAAMoD,UAAU,GAAG1D,MAAM,CAAC2D,MAAP,CAAc,EAAd,EAAkB,KAAKpD,SAAvB,CAAnB;MACAP,MAAM,CAACC,IAAP,CAAYP,MAAZ,EAAoBQ,OAApB,CAA4BC,IAAI,IAAI;QAChC,MAAM,CAACyD,QAAD,EAAWC,KAAX,IAAoBtF,aAAa,CAAC4B,IAAD,CAAvC;QACA,MAAM2D,OAAO,GAAG,EAAhB;QACAA,OAAO,CAACD,KAAD,CAAP,GAAiBnE,MAAM,CAACS,IAAD,CAAvB;QACAuD,UAAU,CAACE,QAAD,CAAV,GAAuBE,OAAvB;MACH,CALD;MAMA,MAAMC,aAAa,GAAG,KAAKC,kBAAL,CAAwBN,UAAxB,CAAtB;MACA,MAAMO,+BAA+B,GAAG,EAAxC;;MACA,KAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGd,YAAY,CAACd,MAAjC,EAAyC4B,CAAC,EAA1C,EAA8C;QAC1C,MAAMnD,IAAI,GAAGqC,YAAY,CAACc,CAAD,CAAzB;;QACA,IAAI,CAACR,UAAU,CAAC3C,IAAI,CAACZ,IAAN,CAAf,EAA4B;UACxB,MAAM2D,OAAO,GAAGtF,SAAS,CAACuC,IAAD,EAAO2C,UAAP,EAAmBD,OAAnB,EAA4B,KAAK3C,gBAAjC,CAAzB;;UACA,IAAI5C,IAAI,CAACiG,SAAL,CAAeL,OAAf,CAAJ,EAA6B;YACzB,MAAM,IAAI1B,KAAJ,CAAW,4BAA2BrB,IAAI,CAACsB,EAAG,wBAApC,GACX,0CADC,CAAN;UAEH;;UACDqB,UAAU,CAAC3C,IAAI,CAACZ,IAAN,CAAV,GAAwB2D,OAAxB;UACA,KAAKM,sBAAL,CAA4BrD,IAAI,CAACZ,IAAjC,EAAuCY,IAAvC,EAA6C2C,UAA7C,EAAyDD,OAAzD,EAAkEM,aAAlE,EAAiFb,eAAjF,EAAkGe,+BAAlG;QACH;MACJ,CAtBa,CAuBd;;;MACA,IAAI,KAAKjF,MAAL,IAAe,IAAnB,EAAyB;QACrByE,OAAO,CAACY,OAAR,CAAgBN,aAAhB;MACH;;MACD,OAAOvE,OAAO,CAACgB,GAAR,CAAYL,IAAI,IAAI9B,SAAS,CAAC8B,IAAD,EAAOuD,UAAP,EAAmBD,OAAnB,CAA7B,CAAP;IACH,CA5BU,CAAX;EA6BH;;EACDO,kBAAkB,CAACM,SAAD,EAAY;IAC1B,MAAMC,GAAG,GAAG,GAAG3D,MAAH,CAAU4D,KAAV,CAAgB,EAAhB,EAAoBxE,MAAM,CAACC,IAAP,CAAYqE,SAAZ,EAC3B9D,GAD2B,CACvBC,GAAG,IAAI6D,SAAS,CAAC7D,GAAD,CADO,EAE3BD,GAF2B,CAEvBsD,OAAO,IAAIA,OAAO,CAACtD,GAAR,CAAYE,MAAM,IAAIA,MAAM,CAACC,EAA7B,CAFY,CAApB,CAAZ;IAGA,OAAO,IAAI8D,GAAJ,CAAQF,GAAR,CAAP;EACH;;EACDH,sBAAsB,CAACR,QAAD,EAAW7C,IAAX,EAAiBuD,SAAjB,EAA4Bb,OAA5B,EAAqCM,aAArC,EAAoDW,WAApD,EAAiET,+BAAjE,EAAkG;IACpH;IACA;IACA,IAAIlD,IAAI,CAAC4D,QAAL,KAAkB,SAAlB,IAA+BD,WAAW,CAACE,OAAZ,CAAoBhB,QAApB,MAAkC,CAAC,CAAtE,EAAyE;MACrE;IACH;;IACDU,SAAS,CAACV,QAAD,CAAT,CAAoB1D,OAApB,CAA4BQ,MAAM,IAAI;MAClC,IAAIA,MAAM,IAAI,IAAd,EAAoB;QAChBuD,+BAA+B,CAACvD,MAAM,CAACC,EAAR,CAA/B,GACI,CAACsD,+BAA+B,CAACvD,MAAM,CAACC,EAAR,CAA/B,IAA8C,CAA/C,IACII,IAAI,CAAC8D,QAAL,CAAcvC,MAFtB;MAGH;IACJ,CAND;IAOAvB,IAAI,CAACrB,MAAL,CAAYQ,OAAZ,CAAoB4E,KAAK,IAAI;MACzB;MACA;MACA,IAAIA,KAAK,CAACH,QAAN,KAAmB,SAAvB,EAAkC;QAC9B,MAAMb,OAAO,GAAGxF,4BAA4B,CAACwG,KAAK,CAAC3E,IAAP,EAAamE,SAAb,EAAwBb,OAAxB,CAA5C;;QACA,IAAIK,OAAO,IAAI,IAAf,EAAqB;UACjBA,OAAO,CAAC5D,OAAR,CAAgBQ,MAAM,IAAI;YACtB,IAAIA,MAAM,IAAI,CAACA,MAAM,CAACqE,IAAlB,IAA0B,CAAChB,aAAa,CAACiB,GAAd,CAAkBtE,MAAM,CAACC,EAAzB,CAA/B,EAA6D;cACzD,MAAMsE,KAAK,GAAGhB,+BAA+B,CAACvD,MAAM,CAACC,EAAR,CAA7C;;cACA,IAAIsE,KAAK,KAAK,CAAd,EAAiB;gBACbvE,MAAM,CAAC2D,OAAP;gBACA,OAAOJ,+BAA+B,CAACvD,MAAM,CAACC,EAAR,CAAtC;cACH,CAHD,MAIK,IAAIsE,KAAK,IAAI,IAAb,EAAmB;gBACpB;gBACA;gBACAhB,+BAA+B,CAACvD,MAAM,CAACC,EAAR,CAA/B;cACH;YACJ;UACJ,CAbD;QAcH;MACJ;IACJ,CAtBD;EAuBH;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACUuE,YAAY,CAACxF,MAAD,EAASF,OAAT,EAAkB;IAAA;;IAAA;MAChC,OAAO,KAAI,CAAC2F,aAAL,CAAmBzF,MAAnB,EAA2BF,OAA3B,CAAP;IADgC;EAEnC;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACU2F,aAAa,CAACzF,MAAD,EAASF,OAAT,EAAkB4F,mBAAmB,GAAG,KAAxC,EAA+C7B,cAAc,GAAG,EAAhE,EAAoEC,aAAa,GAAG,EAApF,EAAwF;IAAA;;IAAA;MACvG,IAAI,CAAC4B,mBAAL,EAA0B;QACtB1F,MAAM,GAAG,MAAI,CAACiD,SAAL,CAAejD,MAAf,CAAT;;QACA,MAAI,CAACmD,WAAL,CAAiBnD,MAAjB;;QACA,MAAI,CAACoD,sBAAL,CAA4BpD,MAA5B;;QACAF,OAAO,GAAG,MAAI,CAACuD,UAAL,CAAgBvD,OAAhB,CAAV;;QACA,MAAI,CAACwD,YAAL,CAAkBxD,OAAlB;MACH;;MACD,MAAMiE,OAAO,GAAG,IAAIhF,gBAAJ,CAAqB,MAAI,CAAC8B,SAA1B,EAAqCgD,cAArC,EAAqDC,aAArD,EAAoE,MAAI,CAAClD,mBAAzE,CAAhB,CARuG,CASvG;MACA;MACA;;MACA,MAAMgE,SAAS,SAAS,MAAI,CAACe,sBAAL,CAA4B3F,MAA5B,EAAoC+D,OAApC,EAA6CjE,OAA7C,EAAsD4F,mBAAtD,CAAxB;MACA,MAAME,OAAO,GAAG9F,OAAO,CAACgB,GAAR,CAAYL,IAAI,IAAI9B,SAAS,CAAC8B,IAAD,EAAOmE,SAAP,EAAkBb,OAAlB,CAA7B,CAAhB,CAbuG,CAcvG;;MACA,MAAM8B,SAAS,GAAGD,OAAO,CAAC9E,GAAR,CAAYgF,CAAC,IAAIA,CAAC,CAAC7E,EAAnB,CAAlB;MACA,MAAM8E,QAAQ,GAAGzF,MAAM,CAACC,IAAP,CAAYP,MAAZ,EAAoBc,GAApB,CAAwBL,IAAI,IAAIT,MAAM,CAACS,IAAD,CAAN,CAAaQ,EAA7C,CAAjB;MACA,MAAM+E,OAAO,GAAG,IAAIjB,GAAJ,CAAQ,CAAC,GAAGc,SAAJ,EAAe,GAAGE,QAAlB,EAA4B,GAAG,MAAI,CAACrF,SAApC,CAAR,CAAhB;MACAJ,MAAM,CAACC,IAAP,CAAYqE,SAAZ,EAAuBpE,OAAvB,CAA+BO,GAAG,IAAI;QAClC,MAAMkF,WAAW,GAAGrB,SAAS,CAAC7D,GAAD,CAA7B;QACAkF,WAAW,CAACzF,OAAZ,CAAoBQ,MAAM,IAAI;UAC1B,IAAIA,MAAM,IAAI,CAACA,MAAM,CAACqE,IAAlB,IAA0B,CAACrE,MAAM,CAACkF,UAAlC,IACA,CAACF,OAAO,CAACV,GAAR,CAAYtE,MAAM,CAACC,EAAnB,CADL,EAC6B;YACzBD,MAAM,CAAC2D,OAAP;UACH;QACJ,CALD;MAMH,CARD,EAlBuG,CA2BvG;;MACA,IAAI,MAAI,CAACrF,MAAL,IAAe,IAAnB,EAAyB;QACrByE,OAAO,CAACY,OAAR,CAAgBqB,OAAhB;MACH;;MACD,OAAOJ,OAAP;IA/BuG;EAgC1G;;EACKO,oBAAoB,CAACnG,MAAD,EAAS6D,cAAT,EAAyBC,aAAzB,EAAwC;IAAA;;IAAA;MAC9D,MAAMsC,YAAY,GAAGpG,MAAM,CAAC+B,MAAP,CAAc,CAACjB,GAAD,EAAME,MAAN,EAAcmD,KAAd,KAAwB;QACvDrD,GAAG,CAAC,MAAI,CAACd,MAAL,CAAYmE,KAAZ,EAAmB1D,IAApB,CAAH,GAA+BO,MAA/B;QACA,OAAOF,GAAP;MACH,CAHoB,EAGlB,EAHkB,CAArB;MAIA,OAAO,MAAI,CAAC2E,aAAL,CAAmBW,YAAnB,EAAiC,MAAI,CAACvE,WAAtC,EAAmD,IAAnD,EAAyDgC,cAAzD,EAAyEC,aAAzE,CAAP;IAL8D;EAMjE;EACD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;EACU6B,sBAAsB,CAAC3F,MAAD,EAAS+D,OAAT,EAAkBiB,WAAlB,EAA+BU,mBAA/B,EAAoD;IAAA;;IAAA;MAC5E,MAAMxC,KAAK,GAAG5C,MAAM,CAACC,IAAP,CAAYP,MAAZ,CAAd;MACA,MAAM2B,UAAU,GAAGuB,KAAK,CAACpC,GAAN,CAAUL,IAAI,IAAI,MAAI,CAACpB,KAAL,CAAWkE,KAAX,CAAiB1E,aAAa,CAAC4B,IAAD,CAAb,CAAoB,CAApB,CAAjB,CAAlB,CAAnB;MACA,MAAM+C,eAAe,GAAGwB,WAAW,CAAClE,GAAZ,CAAgBL,IAAI,IAAI5B,aAAa,CAAC4B,IAAD,CAAb,CAAoB,CAApB,CAAxB,CAAxB;MACA,IAAIoB,WAAW,GAAG2B,eAAe,CAAC1C,GAAhB,CAAoBL,IAAI,IAAI,MAAI,CAACpB,KAAL,CAAWkE,KAAX,CAAiB9C,IAAjB,CAA5B,CAAlB,CAJ4E,CAK5E;;MACA,IAAIoB,WAAW,CAACe,MAAZ,KAAuB,CAA3B,EAA8B;QAC1Bf,WAAW,GAAG,MAAI,CAAChC,QAAnB;MACH;;MACD,MAAM;QAAEwG,SAAF;QAAa9D,aAAb;QAA4BC,WAA5B;QAAyCC;MAAzC,IAAwDzD,oBAAoB,CAACgB,MAAD,EAAS6B,WAAT,EAAsB,MAAI,CAAChB,SAA3B,EAAsC,MAAI,CAACZ,UAA3C,CAAlF,CAT4E,CAU5E;;MACA,MAAMqG,KAAK,GAAG,CACV,GAAG3E,UADO,EACK,GAAG,MAAI,CAACtC,KAAL,CAAWkH,OADnB,EAC4B,IAAI,MAAI,CAACtG,UAAL,IAAmB,EAAvB,CAD5B,EAEZa,GAFY,CAERO,IAAI,IAAI;QACV,OAAO;UAAEA,IAAF;UAAQmF,QAAQ,EAAEzC,OAAO,CAAC0C;QAA1B,CAAP;MACH,CAJa,CAAd;MAKA,MAAMzC,UAAU,GAAG1D,MAAM,CAAC2D,MAAP,CAAc,EAAd,EAAkB,MAAI,CAACpD,SAAvB,CAAnB;MACAP,MAAM,CAACC,IAAP,CAAYP,MAAZ,EAAoBQ,OAApB,CAA4BC,IAAI,IAAI;QAChC,MAAM,CAACyD,QAAD,EAAWC,KAAX,IAAoBtF,aAAa,CAAC4B,IAAD,CAAvC;QACA,MAAM2D,OAAO,GAAG,EAAhB;QACAA,OAAO,CAACD,KAAD,CAAP,GAAiBnE,MAAM,CAACS,IAAD,CAAvB;QACAuD,UAAU,CAACE,QAAD,CAAV,GAAuBE,OAAvB;MACH,CALD;MAMA,MAAMG,+BAA+B,GAAG,EAAxC;;MACA,MAAMF,aAAa,GAAG,MAAI,CAACC,kBAAL,CAAwBN,UAAxB,CAAtB;;MACA,MAAM0C,KAAK,GAAG,EAAd;;MACA,OAAOJ,KAAK,CAAC1D,MAAN,GAAe,CAAtB,EAAyB;QACrB,MAAM+D,QAAQ,GAAG,MAAI,CAACC,YAAL,CAAkBjF,UAAlB,EAA8B2E,KAA9B,EAAqCvC,OAArC,EAA8CC,UAA9C,EAA0D0C,KAA1D,EAAiErC,aAAjE,EAAgFb,eAAhF,EAAiGe,+BAAjG,EAAkI8B,SAAlI,CAAjB;;QACA,MAAMQ,OAAO,CAACC,GAAR,CAAYH,QAAZ,CAAN;MACH;;MACD,IAAInE,WAAW,IAAI,IAAf,IAAuB,CAACkD,mBAA5B,EAAiD;QAC7CqB,OAAO,CAACC,IAAR,CAAc,mEAAD,GACR,gEADL;MAEH;;MACD,MAAMC,cAAc,GAAGpF,WAAW,CAC7BqF,MADkB,CACX7F,IAAI,IAAI,CAACnC,aAAa,CAACmC,IAAD,CAAd,IAChB,CAAC1C,SAAS,CAAC0C,IAAI,CAACZ,IAAN,EAAYuD,UAAZ,EAAwBD,OAAxB,CAFS,EAGlBjD,GAHkB,CAGdO,IAAI,IAAIA,IAAI,CAACZ,IAHC,CAAvB;;MAIA,IAAIwG,cAAc,CAACrE,MAAf,GAAwB,CAA5B,EAA+B;QAC3B,IAAIuE,cAAc,GAAG,EAArB;;QACA,IAAI3E,WAAW,IAAI,IAAnB,EAAyB;UACrB2E,cAAc,GACT,+DAAD,GACK,2BAA0B1E,UAAW,GAF9C;QAGH;;QACD,MAAM,IAAIC,KAAJ,CAAW,+BAA8BuE,cAAe,sBAA9C,GACX,WAAU/D,KAAM,8CADL,GAEX,IAAGX,aAAc,MAAK4E,cAAe,EAFpC,CAAN;MAGH;;MACD,OAAOnD,UAAP;IAjD4E;EAkD/E;;EACD4C,YAAY,CAACjF,UAAD,EAAa2E,KAAb,EAAoBvC,OAApB,EAA6Ba,SAA7B,EAAwC8B,KAAxC,EAA+CrC,aAA/C,EAA8DW,WAA9D,EAA2ET,+BAA3E,EAA4G8B,SAA5G,EAAuH;IAC/H,MAAMM,QAAQ,GAAG,EAAjB;;IACA,OAAOL,KAAK,CAAC1D,MAAN,GAAe,CAAtB,EAAyB;MACrB,MAAMwE,IAAI,GAAGd,KAAK,CAACe,GAAN,EAAb;MACAtD,OAAO,CAAC0C,cAAR,GAAyBW,IAAI,CAACZ,QAA9B;MACA,IAAItC,QAAQ,GAAG,EAAf,CAHqB,CAIrB;MACA;MACA;;MACA,IAAIkD,IAAI,CAAC/F,IAAL,CAAUsB,EAAV,KAAiB,OAAjB,IACAjE,aAAa,CAAC,YAAD,EAAe0I,IAAI,CAAC/F,IAApB,EAA0BuD,SAA1B,EAAqCb,OAArC,CADjB,EACgE;QAC5D,CAACG,QAAD,IAAazF,mBAAmB,CAAC2I,IAAI,CAAC/F,IAAL,CAAUZ,IAAX,EAAiBsD,OAAjB,CAAhC;MACH,CAVoB,CAWrB;MACA;;;MACA,IAAIa,SAAS,CAACwC,IAAI,CAAC/F,IAAL,CAAUZ,IAAX,CAAT,IAA6B,IAAjC,EAAuC;QACnC,MAAM2D,OAAO,GAAGtF,SAAS,CAACsI,IAAI,CAAC/F,IAAN,EAAYuD,SAAZ,EAAuBb,OAAvB,EAAgC,KAAK3C,gBAArC,CAAzB;;QACA,IAAI,CAAC8C,QAAL,EAAe;UACX,CAACA,QAAD,IAAazF,mBAAmB,CAAC2I,IAAI,CAAC/F,IAAL,CAAUZ,IAAX,EAAiBsD,OAAjB,CAAhC;QACH;;QACD,MAAM0C,cAAc,GAAG1C,OAAO,CAAC0C,cAA/B;;QACA,IAAIjI,IAAI,CAACiG,SAAL,CAAeL,OAAf,CAAJ,EAA6B;UACzBuC,QAAQ,CAACW,IAAT,CAAclD,OAAO,CAACmD,IAAR,CAAazB,CAAC,IAAI;YAC5BlB,SAAS,CAACV,QAAD,CAAT,GAAsB4B,CAAtB;YACA/B,OAAO,CAAC0C,cAAR,GAAyBA,cAAzB;YACA,KAAK/B,sBAAL,CAA4BR,QAA5B,EAAsCkD,IAAI,CAAC/F,IAA3C,EAAiDuD,SAAjD,EAA4Db,OAA5D,EAAqEM,aAArE,EAAoFW,WAApF,EAAiGT,+BAAjG;YACA,KAAKiD,iBAAL,CAAuBJ,IAAI,CAAC/F,IAA5B,EAAkCiF,KAAlC,EAAyCvC,OAAzC,EAAkDa,SAAlD,EAA6D8B,KAA7D,EAAoEL,SAApE;YACA,OAAOP,CAAP;UACH,CANa,CAAd;QAOH,CARD,MASK;UACDlB,SAAS,CAACV,QAAD,CAAT,GAAsBE,OAAtB;UACA,KAAKM,sBAAL,CAA4BR,QAA5B,EAAsCkD,IAAI,CAAC/F,IAA3C,EAAiDuD,SAAjD,EAA4Db,OAA5D,EAAqEM,aAArE,EAAoFW,WAApF,EAAiGT,+BAAjG;UACA,KAAKiD,iBAAL,CAAuBJ,IAAI,CAAC/F,IAA5B,EAAkCiF,KAAlC,EAAyCvC,OAAzC,EAAkDa,SAAlD,EAA6D8B,KAA7D,EAAoEL,SAApE;QACH;MACJ,CApBD,MAqBK;QACD,KAAKmB,iBAAL,CAAuBJ,IAAI,CAAC/F,IAA5B,EAAkCiF,KAAlC,EAAyCvC,OAAzC,EAAkDa,SAAlD,EAA6D8B,KAA7D,EAAoEL,SAApE;MACH;IACJ;;IACD,OAAOM,QAAP;EACH;;EACDa,iBAAiB,CAACnG,IAAD,EAAOiF,KAAP,EAAcvC,OAAd,EAAuBa,SAAvB,EAAkC8B,KAAlC,EAAyCL,SAAzC,EAAoD;IACjEhF,IAAI,CAAC8D,QAAL,CAAc3E,OAAd,CAAuBiH,SAAD,IAAe;MACjC,MAAM,CAACvD,QAAD,IAAczF,mBAAmB,CAACgJ,SAAS,CAAChH,IAAX,EAAiBsD,OAAjB,CAAvC;;MACA,IAAI2C,KAAK,CAACxC,QAAD,CAAL,IAAmB,CAACmC,SAAS,CAACf,GAAV,CAAcmC,SAAS,CAAChH,IAAxB,CAAxB,EAAuD;QACnD;MACH,CAJgC,CAKjC;;;MACA,IAAIgH,SAAS,CAAC9E,EAAV,KAAiB,OAArB,EAA8B;QAC1B,IAAI8E,SAAS,CAACC,UAAV,CAAqBC,IAArB,CAA0BlH,IAAI,IAAI;UAClC,OAAO,CAAC,CAAC9B,SAAS,CAAC8B,IAAD,EAAOmE,SAAP,EAAkBb,OAAlB,CAAlB;QACH,CAFG,CAAJ,EAEI;UACA2C,KAAK,CAACxC,QAAD,CAAL,GAAkB,IAAlB;UACAoC,KAAK,CAACgB,IAAN,CAAW;YAAEd,QAAQ,EAAEzC,OAAO,CAAC0C,cAApB;YAAoCpF,IAAI,EAAEoG;UAA1C,CAAX;QACH;MACJ,CAPD,MAQK;QACJ,IAAIA,SAAS,CAACC,UAAV,CAAqBE,KAArB,CAA2BnH,IAAI,IAAI;UACpC,OAAO,CAAC,CAAC9B,SAAS,CAAC8B,IAAD,EAAOmE,SAAP,EAAkBb,OAAlB,CAAlB;QACH,CAFI,CAAJ,EAEG;UACA2C,KAAK,CAACxC,QAAD,CAAL,GAAkB,IAAlB;UACAoC,KAAK,CAACgB,IAAN,CAAW;YAAEd,QAAQ,EAAEzC,OAAO,CAAC0C,cAApB;YAAoCpF,IAAI,EAAEoG;UAA1C,CAAX;QACH;IACJ,CArBD;EAsBH;EACD;AACJ;AACA;;;EACI9C,OAAO,GAAG;IACNrE,MAAM,CAACC,IAAP,CAAY,KAAKM,SAAjB,EACKL,OADL,CACaO,GAAG,IAAI,KAAKF,SAAL,CAAeE,GAAf,EAAoBP,OAApB,CAA4BQ,MAAM,IAAIA,MAAM,CAAC2D,OAAP,EAAtC,CADpB;EAEH;;EACDvB,sBAAsB,CAACpD,MAAD,EAAS;IAC3BM,MAAM,CAACC,IAAP,CAAYP,MAAZ,EAAoBQ,OAApB,CAA4BC,IAAI,IAAI;MAChC,MAAM2E,KAAK,GAAGpF,MAAM,CAACS,IAAD,CAApB;MACA,MAAM,CAACyD,QAAD,IAAcrF,aAAa,CAAC4B,IAAD,CAAjC;MACA,MAAMY,IAAI,GAAG,KAAKhC,KAAL,CAAWkE,KAAX,CAAiBW,QAAjB,CAAb;;MACA,IAAI7C,IAAI,CAACE,UAAL,CAAgB,OAAhB,KAA4BF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAzD,EAAgE;QAC5D,MAAMF,KAAK,GAAGD,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAvC;QACA,MAAMqG,KAAK,GAAGvG,KAAK,CAACsB,MAAN,KAAiBwC,KAAK,CAAC9D,KAAN,CAAYsB,MAA7B,IACVwC,KAAK,CAAC9D,KAAN,CAAYsG,KAAZ,CAAkB,CAACE,GAAD,EAAM3D,KAAN,KAAgB7C,KAAK,CAAC6C,KAAD,CAAL,KAAiB,CAAC,CAAlB,IAAuB7C,KAAK,CAAC6C,KAAD,CAAL,KAAiB2D,GAA1E,CADJ;QAEAtJ,IAAI,CAACuJ,MAAL,CAAYF,KAAZ,EAAmB,MAAO,sBAAqBxG,IAAI,CAACZ,IAAK,iBAAhC,GACpB,gCAA+Ba,KAAM,aADjB,GAEpB,IAAG8D,KAAK,CAAC9D,KAAM,GAFpB;MAGH;;MACD,IAAID,IAAI,CAACE,UAAL,CAAgB,OAAhB,KAA4BF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAzD,EAAgE;QAC5DhD,IAAI,CAACuJ,MAAL,CAAY3C,KAAK,CAAC1D,KAAN,KAAgBL,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAArD,EAA4D,MAAO,sBAAqBH,IAAI,CAACZ,IAAK,iBAAhC,GAC7D,8BAD6D,GAE7D,GAAEY,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAM,aAAY4D,KAAK,CAAC1D,KAAM,EAF9D;MAGH;IACJ,CAjBD;EAkBH;;EACDuB,SAAS,CAACjD,MAAD,EAAS;IACd,MAAMgI,MAAM,GAAG,EAAf;;IACA,KAAK,MAAMC,SAAX,IAAwBjI,MAAxB,EAAgC;MAC5B,IAAI,KAAKG,UAAL,IAAmB,IAAnB,IAA2B,KAAKA,UAAL,CAAgBH,MAAhB,IAA0B,IAArD,IACA,KAAKG,UAAL,CAAgBH,MAAhB,CAAuBiI,SAAvB,KAAqC,IADzC,EAC+C;QAC3C,MAAMjH,MAAM,GAAG,KAAKb,UAAL,CAAgBH,MAAhB,CAAuBiI,SAAvB,CAAf;QACAD,MAAM,CAAChH,MAAM,CAACP,IAAR,CAAN,GAAsBT,MAAM,CAACiI,SAAD,CAA5B;MACH,CAJD,MAKK;QACDD,MAAM,CAACC,SAAD,CAAN,GAAoBjI,MAAM,CAACiI,SAAD,CAA1B;MACH;IACJ;;IACD,OAAOD,MAAP;EACH;;EACD7E,WAAW,CAACnD,MAAD,EAAS;IAChB,MAAMkI,UAAU,GAAG5H,MAAM,CAACC,IAAP,CAAYP,MAAZ,EAAoBkH,MAApB,CAA2BzG,IAAI,IAAI;MAClD,MAAM,CAACyD,QAAD,IAAarF,aAAa,CAAC4B,IAAD,CAAhC;MACA,OAAO,KAAKpB,KAAL,CAAWkE,KAAX,CAAiBW,QAAjB,KAA8B,IAArC;IACH,CAHkB,CAAnB;;IAIA,IAAIgE,UAAU,CAACtF,MAAX,GAAoB,CAAxB,EAA2B;MACvB,MAAM,IAAIF,KAAJ,CAAW,+CAAD,GACX,UAASwF,UAAW,8BADnB,CAAN;IAEH;EACJ;;EACD7E,UAAU,CAACvD,OAAD,EAAU;IAChB,OAAOA,OAAO,CAACgB,GAAR,CAAYL,IAAI,IAAI;MACvB,IAAI,KAAKN,UAAL,IAAmB,IAAnB,IAA2B,KAAKA,UAAL,CAAgBL,OAAhB,IAA2B,IAAtD,IACA,KAAKK,UAAL,CAAgBL,OAAhB,CAAwBW,IAAxB,KAAiC,IADrC,EAC2C;QACvC,MAAMO,MAAM,GAAG,KAAKb,UAAL,CAAgBL,OAAhB,CAAwBW,IAAxB,CAAf;QACA,OAAOO,MAAM,CAACP,IAAd;MACH;;MACD,OAAOA,IAAP;IACH,CAPM,EAOJ,EAPI,CAAP;EAQH;;EACD6C,YAAY,CAACxD,OAAD,EAAU;IAClBA,OAAO,CAACU,OAAR,CAAgBC,IAAI,IAAI;MACpB,MAAM,CAAC0H,cAAD,IAAmBtJ,aAAa,CAAC4B,IAAD,CAAtC;;MACA,IAAI,CAAC,KAAKpB,KAAL,CAAWkE,KAAX,CAAiB4E,cAAjB,CAAL,EAAuC;QACnC,MAAM,IAAIzF,KAAJ,CAAW,eAAcjC,IAAK,6BAA9B,CAAN;MACH;IACJ,CALD;EAMH;;AAvesB"},"metadata":{},"sourceType":"module"}