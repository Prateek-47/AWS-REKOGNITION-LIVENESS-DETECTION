{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\n\nfunction fusedMatMul_({\n  a,\n  b,\n  transposeA = false,\n  transposeB = false,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}) {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedMatMul(a, b, transposeA, transposeB);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  let $a = convertToTensor(a, 'a', 'fused matMul');\n  let $b = convertToTensor(b, 'b', 'fused matMul');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  const outerDimsA = $a.shape.slice(0, -2);\n  const outerDimsB = $b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at ` + `least 2, got ranks ${$a.rank} and ${$b.rank}.`);\n  util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` + `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} must match.`);\n  util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n  const a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) : reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n  const b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) : reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n    [$bias] = makeTypesMatch($bias, $a);\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n\n  const grad = (dy, saved) => {\n    const [a3D, b3D, y, $bias] = saved; // we reshape dy because the result of the forward is not\n    // necessarily going to be a 3d tensor due to a reshape done at the end of\n    // the customOp.\n\n    const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n    let aDer;\n    let bDer;\n\n    if (!transposeA && !transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, true, false);\n    } else if (!transposeA && transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, false);\n      bDer = unfusedMatMul(dyActivation, a3D, true, false);\n    } else if (transposeA && !transposeB) {\n      aDer = unfusedMatMul(b3D, dyActivation, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, false, false);\n    } else {\n      aDer = unfusedMatMul(b3D, dyActivation, true, true);\n      bDer = unfusedMatMul(dyActivation, a3D, true, true);\n    }\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [aDer, bDer, biasDer];\n    } else {\n      return [aDer, bDer];\n    }\n  };\n\n  const inputs = {\n    a: a3D,\n    b: b3D,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    transposeA,\n    transposeB,\n    activation,\n    leakyreluAlpha\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((a3D, b3D, save) => {\n      const res = // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOp(a3D, b3D);\n  } else {\n    const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n      const res = // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res, $bias]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(a3D, b3D, $bias);\n  }\n}\n\nexport const matMul = op({\n  fusedMatMul_\n});","map":{"version":3,"names":["ENGINE","customGrad","_FusedMatMul","makeTypesMatch","convertToTensor","util","add","broadcast_util","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","matMul","unfusedMatMul","op","reshape","fusedMatMul_","a","b","transposeA","transposeB","bias","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","result","$a","$b","innerShapeA","shape","rank","innerShapeB","outerShapeA","outerShapeB","outerDimsA","slice","outerDimsB","batchDimA","sizeFromShape","batchDimB","assert","arraysEqual","outShape","concat","a3D","b3D","$bias","assertAndGetBroadcastShape","$preluActivationWeights","grad","dy","saved","y","dyActivation","aDer","bDer","biasDer","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["D:/aws-rekognition-liveness-detection-main/node_modules/@tensorflow/tfjs-core/dist/ops/fused/mat_mul.js"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha, }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at ` +\n        `least 2, got ranks ${$a.rank} and ${$b.rank}.`);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\n        [$bias] = makeTypesMatch($bias, $a);\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n    const grad = (dy, saved) => {\n        const [a3D, b3D, y, $bias] = saved;\n        // we reshape dy because the result of the forward is not\n        // necessarily going to be a 3d tensor due to a reshape done at the end of\n        // the customOp.\n        const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n        let aDer;\n        let bDer;\n        if (!transposeA && !transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, true, false);\n        }\n        else if (!transposeA && transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, false);\n            bDer = unfusedMatMul(dyActivation, a3D, true, false);\n        }\n        else if (transposeA && !transposeB) {\n            aDer = unfusedMatMul(b3D, dyActivation, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, false, false);\n        }\n        else {\n            aDer = unfusedMatMul(b3D, dyActivation, true, true);\n            bDer = unfusedMatMul(dyActivation, a3D, true, true);\n        }\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [aDer, bDer, biasDer];\n        }\n        else {\n            return [aDer, bDer];\n        }\n    };\n    const inputs = {\n        a: a3D,\n        b: b3D,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { transposeA, transposeB, activation, leakyreluAlpha };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((a3D, b3D, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOp(a3D, b3D);\n    }\n    else {\n        const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res, $bias]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOpWithBias(a3D, b3D, $bias);\n    }\n}\nexport const matMul = op({ fusedMatMul_ });\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,MAAT,QAAuB,cAAvB;AACA,SAASC,UAAT,QAA2B,iBAA3B;AACA,SAASC,YAAT,QAA6B,oBAA7B;AACA,SAASC,cAAT,QAA+B,mBAA/B;AACA,SAASC,eAAT,QAAgC,uBAAhC;AACA,OAAO,KAAKC,IAAZ,MAAsB,YAAtB;AACA,SAASC,GAAT,QAAoB,QAApB;AACA,OAAO,KAAKC,cAAZ,MAAgC,mBAAhC;AACA,SAASC,eAAT,EAA0BC,oBAA1B,EAAgDC,oBAAhD,EAAsEC,UAAtE,QAAwF,eAAxF;AACA,SAASC,MAAM,IAAIC,aAAnB,QAAwC,YAAxC;AACA,SAASC,EAAT,QAAmB,cAAnB;AACA,SAASC,OAAT,QAAwB,YAAxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,SAASC,YAAT,CAAsB;EAAEC,CAAF;EAAKC,CAAL;EAAQC,UAAU,GAAG,KAArB;EAA4BC,UAAU,GAAG,KAAzC;EAAgDC,IAAhD;EAAsDC,UAAU,GAAG,QAAnE;EAA6EC,sBAA7E;EAAqGC;AAArG,CAAtB,EAA8I;EAC1I,IAAIb,UAAU,CAACX,MAAM,CAACyB,KAAP,CAAaC,aAAd,EAA6BJ,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;IAC9D,IAAIK,MAAM,GAAGd,aAAa,CAACI,CAAD,EAAIC,CAAJ,EAAOC,UAAP,EAAmBC,UAAnB,CAA1B;;IACA,IAAIC,IAAI,IAAI,IAAZ,EAAkB;MACdM,MAAM,GAAGrB,GAAG,CAACqB,MAAD,EAASN,IAAT,CAAZ;IACH;;IACD,OAAOb,eAAe,CAACmB,MAAD,EAASL,UAAT,EAAqBC,sBAArB,EAA6CC,cAA7C,CAAtB;EACH;;EACD,IAAII,EAAE,GAAGxB,eAAe,CAACa,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;EACA,IAAIY,EAAE,GAAGzB,eAAe,CAACc,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;EACA,CAACU,EAAD,EAAKC,EAAL,IAAW1B,cAAc,CAACyB,EAAD,EAAKC,EAAL,CAAzB;EACA,MAAMC,WAAW,GAAGX,UAAU,GAAGS,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CAAH,GAA2BJ,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CAAzD;EACA,MAAMC,WAAW,GAAGb,UAAU,GAAGS,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CAAH,GAA2BH,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CAAzD;EACA,MAAME,WAAW,GAAGf,UAAU,GAAGS,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CAAH,GAA2BJ,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CAAzD;EACA,MAAMG,WAAW,GAAGf,UAAU,GAAGS,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CAAH,GAA2BH,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CAAzD;EACA,MAAMI,UAAU,GAAGR,EAAE,CAACG,KAAH,CAASM,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;EACA,MAAMC,UAAU,GAAGT,EAAE,CAACE,KAAH,CAASM,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;EACA,MAAME,SAAS,GAAGlC,IAAI,CAACmC,aAAL,CAAmBJ,UAAnB,CAAlB;EACA,MAAMK,SAAS,GAAGpC,IAAI,CAACmC,aAAL,CAAmBF,UAAnB,CAAlB;EACAjC,IAAI,CAACqC,MAAL,CAAYd,EAAE,CAACI,IAAH,IAAW,CAAX,IAAgBH,EAAE,CAACG,IAAH,IAAW,CAA3B,IAAgCJ,EAAE,CAACI,IAAH,KAAYH,EAAE,CAACG,IAA3D,EAAiE,MAAO,8DAAD,GAClE,sBAAqBJ,EAAE,CAACI,IAAK,QAAOH,EAAE,CAACG,IAAK,GADjD;EAEA3B,IAAI,CAACqC,MAAL,CAAYrC,IAAI,CAACsC,WAAL,CAAiBP,UAAjB,EAA6BE,UAA7B,CAAZ,EAAsD,MAAO,4CAA2CF,UAAW,SAAvD,GACvD,GAAEE,UAAW,4BAA2BV,EAAE,CAACG,KAAM,OADM,GAEvD,GAAEF,EAAE,CAACE,KAAM,cAFhB;EAGA1B,IAAI,CAACqC,MAAL,CAAYZ,WAAW,KAAKG,WAA5B,EAAyC,MAAO,wCAAuCH,WAAY,SAApD,GAC1C,GAAEG,WAAY,4BAA2BL,EAAE,CAACG,KAAM,OADR,GAE1C,GAAEF,EAAE,CAACE,KAAM,mBAAkBZ,UAAW,EAFE,GAG1C,mBAAkBC,UAAW,cAHlC;EAIA,MAAMwB,QAAQ,GAAGhB,EAAE,CAACG,KAAH,CAASM,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,EAAsBQ,MAAtB,CAA6B,CAACX,WAAD,EAAcC,WAAd,CAA7B,CAAjB;EACA,MAAMW,GAAG,GAAG3B,UAAU,GAClBJ,OAAO,CAACa,EAAD,EAAK,CAACW,SAAD,EAAYT,WAAZ,EAAyBI,WAAzB,CAAL,CADW,GAElBnB,OAAO,CAACa,EAAD,EAAK,CAACW,SAAD,EAAYL,WAAZ,EAAyBJ,WAAzB,CAAL,CAFX;EAGA,MAAMiB,GAAG,GAAG3B,UAAU,GAClBL,OAAO,CAACc,EAAD,EAAK,CAACY,SAAD,EAAYN,WAAZ,EAAyBF,WAAzB,CAAL,CADW,GAElBlB,OAAO,CAACc,EAAD,EAAK,CAACY,SAAD,EAAYR,WAAZ,EAAyBE,WAAzB,CAAL,CAFX;EAGA,IAAIa,KAAJ;;EACA,IAAI3B,IAAI,IAAI,IAAZ,EAAkB;IACd2B,KAAK,GAAG5C,eAAe,CAACiB,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;IACA,CAAC2B,KAAD,IAAU7C,cAAc,CAAC6C,KAAD,EAAQpB,EAAR,CAAxB;IACArB,cAAc,CAAC0C,0BAAf,CAA0CL,QAA1C,EAAoDI,KAAK,CAACjB,KAA1D;EACH;;EACD,IAAImB,uBAAJ;;EACA,IAAI3B,sBAAsB,IAAI,IAA9B,EAAoC;IAChC2B,uBAAuB,GAAG9C,eAAe,CAACmB,sBAAD,EAAyB,eAAzB,EAA0C,cAA1C,CAAzC;EACH;;EACD,MAAM4B,IAAI,GAAG,CAACC,EAAD,EAAKC,KAAL,KAAe;IACxB,MAAM,CAACP,GAAD,EAAMC,GAAN,EAAWO,CAAX,EAAcN,KAAd,IAAuBK,KAA7B,CADwB,CAExB;IACA;IACA;;IACA,MAAME,YAAY,GAAG7C,oBAAoB,CAACK,OAAO,CAACqC,EAAD,EAAKE,CAAC,CAACvB,KAAP,CAAR,EAAuBuB,CAAvB,EAA0BhC,UAA1B,CAAzC;IACA,IAAIkC,IAAJ;IACA,IAAIC,IAAJ;;IACA,IAAI,CAACtC,UAAD,IAAe,CAACC,UAApB,EAAgC;MAC5BoC,IAAI,GAAG3C,aAAa,CAAC0C,YAAD,EAAeR,GAAf,EAAoB,KAApB,EAA2B,IAA3B,CAApB;MACAU,IAAI,GAAG5C,aAAa,CAACiC,GAAD,EAAMS,YAAN,EAAoB,IAApB,EAA0B,KAA1B,CAApB;IACH,CAHD,MAIK,IAAI,CAACpC,UAAD,IAAeC,UAAnB,EAA+B;MAChCoC,IAAI,GAAG3C,aAAa,CAAC0C,YAAD,EAAeR,GAAf,EAAoB,KAApB,EAA2B,KAA3B,CAApB;MACAU,IAAI,GAAG5C,aAAa,CAAC0C,YAAD,EAAeT,GAAf,EAAoB,IAApB,EAA0B,KAA1B,CAApB;IACH,CAHI,MAIA,IAAI3B,UAAU,IAAI,CAACC,UAAnB,EAA+B;MAChCoC,IAAI,GAAG3C,aAAa,CAACkC,GAAD,EAAMQ,YAAN,EAAoB,KAApB,EAA2B,IAA3B,CAApB;MACAE,IAAI,GAAG5C,aAAa,CAACiC,GAAD,EAAMS,YAAN,EAAoB,KAApB,EAA2B,KAA3B,CAApB;IACH,CAHI,MAIA;MACDC,IAAI,GAAG3C,aAAa,CAACkC,GAAD,EAAMQ,YAAN,EAAoB,IAApB,EAA0B,IAA1B,CAApB;MACAE,IAAI,GAAG5C,aAAa,CAAC0C,YAAD,EAAeT,GAAf,EAAoB,IAApB,EAA0B,IAA1B,CAApB;IACH;;IACD,IAAIzB,IAAI,IAAI,IAAZ,EAAkB;MACd,MAAMqC,OAAO,GAAGjD,oBAAoB,CAACuC,KAAD,EAAQO,YAAR,CAApC;MACA,OAAO,CAACC,IAAD,EAAOC,IAAP,EAAaC,OAAb,CAAP;IACH,CAHD,MAIK;MACD,OAAO,CAACF,IAAD,EAAOC,IAAP,CAAP;IACH;EACJ,CA/BD;;EAgCA,MAAME,MAAM,GAAG;IACX1C,CAAC,EAAE6B,GADQ;IAEX5B,CAAC,EAAE6B,GAFQ;IAGX1B,IAAI,EAAE2B,KAHK;IAIXzB,sBAAsB,EAAE2B;EAJb,CAAf;EAMA,MAAMU,KAAK,GAAG;IAAEzC,UAAF;IAAcC,UAAd;IAA0BE,UAA1B;IAAsCE;EAAtC,CAAd,CAnF0I,CAoF1I;EACA;;EACA,IAAIH,IAAI,IAAI,IAAZ,EAAkB;IACd,MAAMwC,QAAQ,GAAG5D,UAAU,CAAC,CAAC6C,GAAD,EAAMC,GAAN,EAAWe,IAAX,KAAoB;MAC5C,MAAMC,GAAG,GACT;MACA/D,MAAM,CAACgE,SAAP,CAAiB9D,YAAjB,EAA+ByD,MAA/B,EAAuCC,KAAvC,CAFA;MAGAE,IAAI,CAAC,CAAChB,GAAD,EAAMC,GAAN,EAAWgB,GAAX,CAAD,CAAJ;MACA,OAAO;QAAEE,KAAK,EAAElD,OAAO,CAACgD,GAAD,EAAMnB,QAAN,CAAhB;QAAiCsB,QAAQ,EAAEf;MAA3C,CAAP;IACH,CAN0B,CAA3B;IAOA,OAAOU,QAAQ,CAACf,GAAD,EAAMC,GAAN,CAAf;EACH,CATD,MAUK;IACD,MAAMoB,gBAAgB,GAAGlE,UAAU,CAAC,CAAC6C,GAAD,EAAMC,GAAN,EAAWC,KAAX,EAAkBc,IAAlB,KAA2B;MAC3D,MAAMC,GAAG,GACT;MACA/D,MAAM,CAACgE,SAAP,CAAiB9D,YAAjB,EAA+ByD,MAA/B,EAAuCC,KAAvC,CAFA;MAGAE,IAAI,CAAC,CAAChB,GAAD,EAAMC,GAAN,EAAWgB,GAAX,EAAgBf,KAAhB,CAAD,CAAJ;MACA,OAAO;QAAEiB,KAAK,EAAElD,OAAO,CAACgD,GAAD,EAAMnB,QAAN,CAAhB;QAAiCsB,QAAQ,EAAEf;MAA3C,CAAP;IACH,CANkC,CAAnC;IAOA,OAAOgB,gBAAgB,CAACrB,GAAD,EAAMC,GAAN,EAAWC,KAAX,CAAvB;EACH;AACJ;;AACD,OAAO,MAAMpC,MAAM,GAAGE,EAAE,CAAC;EAAEE;AAAF,CAAD,CAAjB"},"metadata":{},"sourceType":"module"}